\chapter{Results and Evaluation}

%\subsection{1D Evaluation \label{evaluation1D}}

Parameters used in evaluation
\begin{itemize}
  \item viewpoint density / ``resolution'' of captures
  \item distance of scene from model (checkersphere, square room, arbitrary room)
  \item number of samples/viewpoints used for interpolation \ar two vs all within semi-large radius (all would be very computation intensive)
  \item flow-based blending vs deviation-angle-based blending
  \item only makes sense to compare if flow algorithm works more or less correctly \ar narrows the parameter space for comparison
\end{itemize}

Fixed params:
\begin{itemize}
  \item radius accuracy
  \item weight metric only deviation angle, not distance (although distance is limited by viewpoint choice)
  \item only 2D
  \item metadata is present and correct
\end{itemize}

limits of the evaluation:
\begin{itemize}
  \item objects are not clearly classified/measured, so no quantifiable evaluation possible
  \item positional and rotational knowledge may be calculable by sfm algos, but we don't know what kind of impact accuracy of the metadata will have, since we are hand-recording metadata meticulously
  \item only measuring at center and corners, which should be the extreme cases. no data on intermediate cases
\end{itemize}

Assuming radius accuracy does in fact make a slight assumption about the scene geometry. Using only the deviation angle will lead to ``spots'' where the distance is fairly large but the angle is zero. 

Future work based on fixed params:
\begin{itemize}
  \item ``guess'' an optimal radius without using viewpoint locations e.g. outside
  \item find a good weight function that balances deviation angle and distance appropriately
  \item figure out a good way to find a ``left and right'' for 3D, also make it more stable for 2D (so that viewpoints are chosen that are relatively close to one another so that the optical flow algorithm works better)
  \item use methods like SLAM in order to be independent of actually recording metadata by hand
\end{itemize}


Process:
\begin{itemize}
  \item first, for each scene, determine resolution for which optical flow still works by using 1D interpolation 
  \item diagonal movement, left to right, front to back for each resolution setting
  \item interpolation in 0.1 steps \ar render out (or record) correct values
  \item where in the scene? close to the walls? in the middle?
\end{itemize}

\section{Optical Flow}
\begin{itemize}
  \item optical flow ground truth is impossible to get from real scenes
  \item however, virtual scenes contain all necessary information for retrieving ground truth for optical flow
  \item virtual camera rig that captures one image per ``side'' with a fov that corresponds to that used in the ExtendedCubeMap \ar extended flow cube
\end{itemize}

"correctness" of optical flow interpolation is limited even with perfect optical flow:
\begin{itemize}
   \item points that are not visible because of perspective shift will not have a correspondence
\end{itemize}
errors that are unrelated to the algorithm:
\begin{itemize}
  \item slight displacement due to ExtendedCubeMap
  \item black edges due to latlong-cube conversion
  \item these need to be taken into account (normalized out)
\end{itemize}
