%\chapter{Proof of Concept Design and Implementation}
\chapter{Pixel-based Synthesis of 360\degree Viewpoints}
\begin{itemize}
  \item most other approaches rely on either geometry or correspondences
  \item usually some form of triangulation
  \item use of sphere for ray tracing, then use texture lookup to find the pixel values
  \item combination of reprojection ie warping with angle constraint and flow-based blending
  \item first part relies on radius, but no other geometry, second part relies on image correspondences
  \item pixel-based, in that each pixel is calculated separately, there are not image-area based constraints
\end{itemize}

methodology
\begin{itemize}
\item two distinct approaches: for 1D interpolation and for 2D/3D interpolation
\item 1D interpolation: interpolate along a line between two viewpoints
\item 2D interpolation: viewpoints are all taken on a plane \ar synthesize a new viewpoint anywhere on the plane
\item 3D interpolation: viewpoints are in 3D space \ar a new viewpoint can be synthesized anywhere within this space
\missingfigure{representation of 1D, 2D and 3D interpolation}
\end{itemize}

\section{Flow-based Interpolation for 1DoF}
The simplest type of synthesis in the scope of this thesis is the creation of a new viewpoint that lies exactly on a line between two existing viewpoints. Assuming that the existing viewpoints are facing the same direction, the line describes a transformation with one degree of freedom, i.e. movement along the line. A solution to a similar problem has already been published by Richardt et al. in ``Megastereo'' \cite{megastereo} which introduces an approach using optical flow in order to reduce ghosting artefacts in planar images. The following section shows how the problem of 1D interpolation for 360\degree images can be reduced to the problem solved in Megastereo.

\subsection{Flow-based Blending in Megastereo}
Megastereo aims to generate high-resolution stereo panoramas by combining images captured on a circle. Their approach is to combine corresponding strips of the captured images and to create a view for each eye (see section \ref{megastereo}). In order to mitigate artefacts such as ghosting, they use ``flow-based blending'' to combine two images A and B. This consists of using the optical flow vectors $F_{A\rightarrow B}$ and their inverse $F_{B\rightarrow A}$. To get the interpolated image at position $\alpha$ between image A and B, first, image A is shifted by $\alpha \cdot F_{A\to B}$ and image B is shifted by $(1 - \alpha) \cdot F_{A\to B}$, yielding $I_A$ and $I_B$, respectively. Then, $I_A$ is multiplied by $(1-\alpha)$ and $I_B$ by $\alpha$ and these pixel values are added together to give the resulting interpolation. This is described by the following function, in which each pixel at position x is defined by: 
\begin{align}
S(x) = (1-\alpha ) \cdot I_A( x + \alpha \cdot F_{A\to B}(x) \\
     + \alpha \cdot I_B( x + (1-\alpha) \cdot F_{B\to A}(x))
\end{align}

\subsection{Reducing 360\degree Interpolation to Planar Interpolation}

A 360\degree image can be projected in several ways, as described in Section \ref{projections}. The output of these projections is a planar image, meaning that flow-based blending could be applied directly. However, this would not lead to optimal results for several reasons: Most projections distort the image in some areas, which would result in distorted optical flow values. Furthermore, in order to make planar viewing possible, the 360\degree image must be ``unfolded'' along some seam. When calculating optical flow, points that move across the seam will not be tracked, even though they do not move out of the image space, as would happen in a regular image.
\missingfigure{points moving across edge in regular vs 360\degree image}

Of the existing projections, only two really come into consideration for this problem: the equirectangular representation and the cubemap representation. Spherical representations are impractical, as aligning seams is not feasible. The equirectangular representation has fewer seams to handle, but also distorts the image greatly around the poles. The cubemap representation hardly distorts the image at all, but contains a number of seams. In this case, the distortion is more difficult to deal with, since the effects on the optical flow algorithm are unclear. Handling seams is more straightforward, which is why the cubemap representation was chosen for 1D interpolation.

In order to handle points moving across seams, the optical flow algorithm must be able to read data \emph{across the seams}. For example, when calculating optical flow on the ``front'' face, data from the ``top'', ``left'', ``right'' and ``bottom'' faces is required. Intuitively, one might just assemble these faces in a plus shape, with the ``front'' face at the center. However, this is a fallacy, since the cubemap projection uses a different virtual camera for each face, which means that linearity is not preserved for points moving across seams.

\missingfigure{plus shape with point moving across seam}

To solve this problem, the ``extended cube map'' is introduced, which was also used by Huang et. al. \cite{6dof}. Instead of projecting a field of view of 90\degree for each camera, which covers 360\degree of the image, the extended cube map uses a larger field of view for each camera. As a result, the areas of the image that are split by a seam are represented twice: Once on each face that is adjacent to the seam. This way, when calculating optical flow on each face separately, points that move across where the seam would be in a regular cube map remain on the face with the corresponding projection. 

Naturally, this method is limited by the field of view used by the virtual cameras. If the maximum displacement is larger than the face extension, the extended cube map will not be sufficient, which will result in black edges on the faces. Also, the larger the field of view, the more the image will be distorted towards the edges of a face, which may lead to distorted optical flow results. This means that displacement between two images is limited. However, the displacement that is trackable by optical flow algorithms is also limited. The effect of these limitations will be explored in section \ref{evaluation1D}.

Using the extended cube map, it is now possible to process each face separately, meaning that the 360\degree 1D interpolation has been reduced to planar images. At the end of the interpolation step, the extended cube map must be clipped back to the original cube map size.
\missingfigure{1D interpolation process diagram}

further points to include
\begin{itemize}
\item no information on position needed, images should be rotated to face the same direction for best results, otherwise the rotation from A to B will also be interpolated
\item inverting flow is non-trivial
\item optical flow parameter choice (maybe in background chapter)
\end{itemize}


\section{2D/3D Interpolation}
The main difference between 1DoF and 2/3DoF synthesis is that in 1DoF interpolation, the choice of viewpoints is trivial, as there are only two by definition. In 2/3DoF synthesis, the idea is to have an unlimited number of viewpoints as input (for 2DoF synthesis, these viewpoints must all be on a plane). Since all the images are 360\degree images, each one theoretically captures the complete surroundings. This means that every existing point should be present somewhere in each image (ignoring occlusions at the moment).
%The challenge in 2DoF synthesis is to find the ``most fitting'' capture for each point of the synthesized image and then rearrange these found points so that they are as close as possible to the image captured at our desired viewpoint. 
The basic idea of the 2/3DoF algorithm is to find the image areas from the set of existing viewpoints that are the ``most fitting'' for the synthesized viewpoint and transform these areas to approximate where they would be in the synthesized image. This means that a metric is necessary that measures how ``fitting'' a specific pixel of a viewpoint image is. Also, a reprojection needs to be found that transforms the ``fitting'' image areas to the appropriate image coordinates for the synthesized viewpoint.
\missingfigure{point correspondences and reprojection intro}

\subsection{Distance metric}
In order to find the ``most fitting'' image area from all of the viewpoint images, there first needs to be a metric that measures how ``fitting'' an image area is. 

One example is the euclidean distance of the viewpoint locations in space. This would mean that for each pixel, the corresponding pixel of the \emph{nearest viewpoint} would be used. This is the simplest approach, and would simply return the nearest neighbor. 

\begin{itemize}
    \item note: algorithm designed for 3D, which can be reduced to 2D, so from now on use of 3D unless otherwise specified
    \item reduction to planar images no longer possible, since the viewpoint selection is no longer trivial \ar use different image for different parts of the image?
    \item which viewpoints to use for which areas of the synthesized image?
\end{itemize}

\subsection{Basic Idea}



\begin{itemize}
    \item all viewpoints more or less capture complete surroundings from different perspectives \ar each point from an input viewoint will be somewhere on the synthesized viewpoint
    \item find corresponding points with the output viewpoint for each input viewpoint (\emph{``where would point P (seen in viewpoint A) be in the synthesized viewpoint S''})
    \item to determine which viewpoints to use for which output image areas, use weighting function $W(viewpoint)$ that is dependent on specific properties of the input viewpoint (e.g. euclidean distance of locations)
    \item in order to do all this, need scene model which is a sphere since we have no geometry information
\end{itemize}

\subsection{Implementation}

\subsubsection{Preprocessing and CaptureSet}
\begin{itemize}
  \item rotate all images so that they all have the same orientation
  \item center point cloud
  \item find appropriate sphere radius
  \item make access to location and image data of a viewpoint contained in a set easy and intuitive
\end{itemize}

\subsubsection{Interpolation}
\begin{itemize}
  \item ray and scene intersection
  \item viewpoint reprojection
  \item deviation angle calculation
  \item weighting function
\end{itemize}

\subsection{Debugging and Verification}
\begin{itemize}
  \item in order to verify correct results and check for bugs, use of the ``checkersphere'' in which the scene model exactly matches the scene \ar reprojection will yield exact results (excluding resolution)
  \item explain why it will yield correct results and why the resolution is not perfect
\end{itemize}

\section{Combining 1D and 2D/3D}
\subsection{Implementation challenges}
``one on each side''
\begin{itemize}
  \item define ``on either side'' for angles: split the circle in two 180\degree halves; one is positive, the other is negative
  \item selection of a viewpoint ``on either side'' \ar what if the viewpoint on one side has a very large deviation angle or does not exist?
  \item can it not exist if we are looking at the convex hull? Yes, if the point is exctly on the border
  \item possible solution: if none is found, use only one side
  \item what to do with zero degree difference? \ar only use zero, not the other (should be solved by the function that defines the 1D interpolation position)
\end{itemize}
using only 2D data
\begin{itemize}
  \item ``left and right'' only possible in 2D (can use signed angles)
  \item in 3D, finding viewpoint on ``either side'' becomes a lot more complex
  \item reducing viewpoints to 2D is simple, but scene model is still in 3D. simplifying scene model to 2D as well
\end{itemize}
``ground truth optical flow by blender''
\begin{itemize}
  \item ``misuse'' of vector render layer
\end{itemize}
