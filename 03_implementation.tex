\chapter{Proof of Concept Design and Implementation}
\begin{itemize}
\item two distinct approaches: for 1D interpolation and for 2D/3D interpolation
\item 1D interpolation: interpolate along a line between two viewpoints
\item 2D interpolation: viewpoints are all taken on a plane \ar synthesize a new viewpoint anywhere on the plane
\item 3D interpolation: viewpoints are in 3D space \ar a new viewpoint can be synthesized anywhere within this space
\missingfigure{representation of 1D, 2D and 3D interpolation}
\end{itemize}

\section{1D Interpolation}
The simplest type of synthesis in the scope of this thesis is the creation of a new viewpoint that lies exactly on a line between two existing viewpoints. Assuming that the existing viewpoints are facing the same direction, the line describes a transformation with one degree of freedom, i.e. movement along the line. A solution to a similar problem has already been published by Richardt et al. in ``Megastereo'' \cite{megastereo} which introduces an approach using optical flow in order to reduce ghosting artefacts in planar images. The following section shows how the problem of 1D interpolation for 360\degree images can be reduced to the problem solved in Megastereo.

\subsection{Flow-based Blending in Megastereo}
Megastereo aims to generate high-resolution stereo panoramas by combining images captured on a circle. Their approach is to interpolate between two adjacent images in order to synthesize any image on the line between the two images. In order to \ldots

\subsection{Reducing 360\degree Image Synthesis to Planar Image Synthesis}

A 360\degree image can be projected in several ways, as described in Section \ref{projections}. The output of these projections is a planar image, meaning that flow-based blending could be applied directly. However, this would not lead to optimal results for several reasons: Most projections distort the image in some areas, which would result in distorted optical flow values. Furthermore, in order to make planar viewing possible, the 360\degree image must be ``unfolded'' along some seam. When calculating optical flow, points that move across the seam will not be tracked, even though they do not move out of the image space, as would happen in a regular image.
\missingfigure{points moving across edge in regular vs 360\degree image}

Of the existing projections, only two really come into consideration for this problem: the equirectangular representation and the cubemap representation. Spherical representations are impractical, as aligning seams is not feasible. The equirectangular representation has fewer seams to handle, but also distorts the image greatly around the poles. The cubemap representation hardly distorts the image at all, but contains a number of seams. In this case, the distortion is more difficult to deal with, since the effects on the optical flow algorithm are unclear. Handling seams is more straightforward, which is why the cubemap representation was chosen for 1D interpolation.

In order to handle points moving across seams, the optical flow algorithm must be able to read data \emph{across the seams}. For example, when calculating optical flow on the ``front'' face, data from the ``top'', ``left'', ``right'' and ``bottom'' faces is required. Intuitively, one might just assemble these faces in a plus shape, with the ``front'' face at the center. However, this is a fallacy, since the cubemap projection uses a different virtual camera for each face, which means that linearity is not preserved for points moving across seams.

\missingfigure{plus shape with point moving across seam}

To solve this problem, the ``extended cube map'' is introduced, which was also used by Huang et. al. \cite{6dof}. Instead of projecting a field of view of 90\degree for each camera, which covers 360\degree of the image, the extended cube map uses a larger field of view for each camera. As a result, the areas of the image that are split by a seam are represented twice: Once on each face that is adjacent to the seam. This way, when calculating optical flow on each face separately, points that move across where the seam would be in a regular cube map remain on the face with the corresponding projection. 

Naturally, this method is limited by the field of view used by the virtual cameras. If the maximum displacement is larger than the face extension, the extended cube map will fail. Also, the larger the field of view, the more the image will be distorted towards the edges of a face, which may lead to distorted optical flow results. This means that displacement between two images is limited. However, the displacement that is trackable by optical flow algorithms is also limited. The effect of these limitations will be explored in section \ref{evaluation1D}.

The optical flow calculation on the extended cube map yields flow vectors describing how each pixel of each face moves from the first to the second image. This information can then be used to interpolate between the two images using flow-based blending.


further points to include
\begin{itemize}
\item no information on position needed, images should be rotated to face the same direction for best results, otherwise the rotation from A to B will also be interpolated
\item viewpoint "selection" is trivial, as there are only two
\item inverting flow is non-trivial
\end{itemize}


\subsection{Optical Flow Parameter Choice}
Parameters need to be chosen based on the size of the images (?)


\section{2D/3D Interpolation}

\begin{itemize}
    \item note: algorithm designed for 3D, which can be reduced to 2D, so from now on use of 3D unless otherwise specified
    \item every viewpoint theoretically has information about every point in the scene (because the captures are 360\degree images)
    \item synthesized viewpoint can be anywhere in 3D space \ar no longer an interpolation between two viewpoints
    \item reduction to planar images no longer possible, since the viewpoint selection is no longer trivial \ar use different image for different parts of the image?
    \item which viewpoints to use for which areas of the synthesized image?
\end{itemize}

\subsection{Basic Idea}
\begin{itemize}
    \item all viewpoints more or less capture complete surroundings from different perspectives \ar each point from an input viewoint will be somewhere on the synthesized viewpoint
    \item find corresponding points with the output viewpoint for each input viewpoint (\emph{``where would point P (seen in viewpoint A) be in the synthesized viewpoint S''})
    \item to determine which viewpoints to use for which output image areas, use weighting function $W(viewpoint)$ that is dependent on specific properties of the input viewpoint (e.g. euclidean distance of locations)
    \item in order to do all this, need scene model which is a sphere since we have no geometry information
\end{itemize}

\subsection{Implementation}

\subsubsection{Preprocessing and CaptureSet}
\begin{itemize}
  \item rotate all images so that they all have the same orientation
  \item center point cloud
  \item find appropriate sphere radius
  \item make access to location and image data of a viewpoint contained in a set easy and intuitive
\end{itemize}

\subsubsection{Interpolation}
\begin{itemize}
  \item ray and scene intersection
  \item viewpoint reprojection
  \item deviation angle calculation
  \item weighting function
\end{itemize}

\subsection{Debugging and Verification}
\begin{itemize}
  \item in order to verify correct results and check for bugs, use of the ``checkersphere'' in which the scene model exactly matches the scene \ar reprojection will yield exact results (excluding resolution)
  \item explain why it will yield correct results and why the resolution is not perfect
\end{itemize}

\section{Combining 1D and 2D/3D}
\subsection{Implementation challenges}
``one on each side''
\begin{itemize}
  \item define ``on either side'' for angles: split the circle in two 180\degree halves; one is positive, the other is negative
  \item selection of a viewpoint ``on either side'' \ar what if the viewpoint on one side has a very large deviation angle or does not exist?
  \item can it not exist if we are looking at the convex hull? Yes, if the point is exctly on the border
  \item possible solution: if none is found, use only one side
  \item what to do with zero degree difference? \ar only use zero, not the other (should be solved by the function that defines the 1D interpolation position)
\end{itemize}
using only 2D data
\begin{itemize}
  \item ``left and right'' only possible in 2D (can use signed angles)
  \item in 3D, finding viewpoint on ``either side'' becomes a lot more complex
  \item reducing viewpoints to 2D is simple, but scene model is still in 3D. simplifying scene model to 2D as well
\end{itemize}<++>
``ground truth optical flow by blender''
\begin{itemize}
  \item ``misuse'' of vector render layer
  \item problem: in order to get the correct optical flow, the scene needs to move, not the camera \ldots ugh
\end{itemize}<++>
