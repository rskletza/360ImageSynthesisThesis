\chapter{Background and Related Work}

%\section{Terminology \ar Glossary?}
%
%\paragraph{Planar image}
%\paragraph{360\degree image} vs panorama
%\paragraph{Interpolation vs Synthesis} In the context of this thesis, the two terms are very similar. Interpolation is used as a type of synthesis where two images are weighted by a certain amount and added together \ldots ?
%\paragraph{Viewpoint, capture, camera} input viewpoint, output viewpoint
%Capture/Viewpoint is the term used here to signify the location and image data of a 360\degree photograph taken at a certain position in the scene
%capture is more for when the images are actually recorded
%viewpoint is more for during processing
%\paragraph{UV coordinates}
%image-based rendering vs pixel-based rendering
\todo{intro}
%\section{Fundamentals of 360\degree Images}
\section{Fundamentals}
Before diving into image-based rendering techniques in general, and the pixel-based synthesis presented in this thesis, it is necessary to outline some basic concepts and methods used in these techniques. Since the images used in this thesis are 360\degree images, it is important to understand how such images are captured, how the captured data can be mapped to a planar surface, and what the most common mappings for 360\degree images are (Section~\ref{subsec:fundamentals_360}). Also, the concept of optical flow is introduced, as it is a prerequisite for a number of image-based rendering techniques, including the pixel-based approach presented in this thesis.

\subsection{360\degree Images}\label{subsec:fundamentals_360}
Capturing an image with a 360\degree camera differs significantly from capturing an image with a regular camera. A regular camera captures incoming rays of light with a limited field of view. The sensors on the camera (or the film, for analog cameras), are arranged on a plane and register the wavelengths of incoming rays. This process represents a projection of the scene onto a plane. The measured light values can then be stored directly as a planar image (Figure~\ref{fig:cameras}a).

A 360\degree (or ``omnidirectional'') camera, on the other hand, captures light rays with an field of view of 360$^{\circ}$. This means that the sensors are arranged in a way that captures light rays from the entire surroundings. For the sake of simplicity, this can be pictured as a number of sensors in the form of a sphere\footnotemark. The camera must then perform an additional conversion in order to transform the light values captured on a sphere to a planar image (Figure~\ref{fig:cameras}b).

\footnotetext{The sensors cannot actually take the form of a perfect sphere, since the camera needs to have some form of casing. Instead, several lenses are usually used, and the image stitched together in software.}

\begin{figure}
\centering
    \begin{subfigure}[t]{0.9\textwidth}            
            \centering
            \includegraphics[width=\textwidth]{02/schematic01.png}
            \caption{Image capturing process with a regular camera: The sensors are arranged on a plane, so capturing the rays corresponds to a projection to a plane.}
    \end{subfigure}
    \begin{subfigure}[t]{0.9\textwidth}
            \centering
            \includegraphics[width=\textwidth]{02/schematic02.png}
            \caption{Image capturing process with a 360\degree camera: The sensors are arranged in an approximation of a sphere, which means an additional step is needed to convert the captured data to a planar representation.}
    \end{subfigure}
    \caption{Capturing an image with a regular camera compared to a 360\degree camera}\label{fig:cameras}
\end{figure}

%\subsubsection{UV Mapping for Spherical Geometry}
The projection onto a flat surface is necessary, since image data is generally stored in 2D, and the majority of viewing devices are planar (e.g. computer or smartphone screens). The process of translating data from a 3D model to a 2D image and vice versa is well known in computer graphics and is called \emph{uv mapping} or \emph{texture mapping}.

Specifically, the process of uv mapping for spherical geometry is needed for mapping the data from the sphere to a planar image. This process describes a bijective operation in which the points (x,y,z) on the sphere (described by \emph{unit directions} which are unit vectors) are associated with pixel positions in image coordinates (u,v). Figure~\ref{fig:uv_mapping} shows an example mapping between the unit sphere and a planar image. In this example, the poles of the sphere are mapped to the entire top and bottom pixel row of the image, and the equator is mapped to the row of pixels in the vertical center of the image. This means that the areas near the poles are \emph{oversampled}, which indicates that the mapping function is not \emph{equal area} \cite[p.450]{hdrbook} i.e. it does not conserve how much area a pixel value occupies.

\begin{figure}
		\centering
		\includegraphics[width=0.8\textwidth, keepaspectratio]{02/uv_mapping.png}
		\caption[UV mapping example]{Example of uv mapping for spherical geometry}
		\label{fig:uv_mapping}
\end{figure}

In the case of a 360\degree camera mapping the captured light rays to a planar image for storage, the image values are some type of color values. However, other kinds of information can also be uv-mapped to a shape, for example illumination data, depth values (bump mapping), and more.

%In the case of 360\degree images, the first step after capture is to convert the captured color values to a a planar image for storage and viewing.
%There are a number of common different mappings from which to choose from, depending on the application.
%\subsubsection{Common Mappings for 360\degree Images \cite[p. 535ff]{hdrbook}}\label{subsec:projections}
The most common mappings for 360\degree images are the \emph{cube map}, the \emph{ideal mirrored sphere}, the \emph{angular map},  and the \emph{equirectangular map} \cite[p. 535]{hdrbook}. The image data can be projected using any of these mappings with only minimal data loss (by interpolation). These mappings are briefly presented in the following paragraphs.

\paragraph{Cube Map}
The cube map is a mapping that splits the image data into six separate square views, one in each direction (top, front, left, right, back, bottom). This is the equivalent of capturing the surroundings with six different cameras with a field of view of 90\degree each, and then stitching the resulting images into a shape that can be ``folded'' into a cube (see Figure~\ref{fig:cubemap-intro}), which also gives this mapping its name.

Due to the projection of a spherical surface to a plane, there is some distortion towards the edges of each face. However, this distortion is comparable to the distortion at the edges of a regular, planar image, which is a significant advantage compared to other mappings (see Figure\ref{fig:common_mappings}b,d,f,h\footnotemark). The disadvantage is that each face is projected separately, which leads to directional discontinuities at the many seams. This type of mapping is often used to simulate complex environments in 3D scenes (e.g. for game or animation graphics), as it is easy to use and reduces render time significantly compared to a 3D model of the same environment.

\footnotetext{Figure~\ref{fig:cubemap-distortion} does not perfectly represent the distortion in cube maps. It was chosen as a baseline because cube maps have relatively small distortion compared to other mappings and it visualizes clearly which parts of the image are mapped where and how they are distorted in other mappings.}
\cite[p. 540]{hdrbook}

\paragraph{Ideal Mirrored Sphere}
The ideal mirrored sphere is a mapping to a circle within a square. It represents how the surroundings would be reflected if one placed a small sphere with a perfectly reflective surface (``mirrored'' sphere) into a scene and then photographed it using an orthographic camera. This mapping, like all the mappings presented here, shows the complete surroundings, albeit very distorted toward the edges. Figure~\ref{fig:sphere-intro}b shows where each direction is mapped and the extent of the distortion. It is clear that the farther away from the ``front'' area, the more distorted the mirrored sphere mapping is. The ideal mirrored sphere mapping can be used for calculating average illumination color for high dynamic range calculations, however, the type of distortion at the edges can cause problems with sampling, which is why the angular map mapping tends to be preferred.
\cite[p. 535]{hdrbook}

\paragraph{Angular Map}
At first sight, the angular map seems very similar to the ideal mirrored sphere. It also maps to a circle within a square, however it samples the input in such a way that the back of the image is allotted more space and is less distorted than the mirrored sphere (see Figure~\ref{fig:angular-intro}).
\cite[p. 537]{hdrbook}

\paragraph{Equirectangular Map}
The equirectangular, or latitude-longitude (latlong) mapping is a common type of mapping in cartography. The data is mapped to a rectangular image space, in which the width is twice the height. The azimuth (around the circumference) of the unit directions is mapped to the map's horizontal coordinate and the elevation to the vertical coordinate. The main problem of this representation is well known in cartography: The distortion increases significantly towards the poles, as can be seen in Figure~\ref{fig:latlong-intro}. Otherwise, this mapping is convenient as it has very few seams and all pixels are valid (i.e. no ``black'' areas). It is used as a storage format for 360\degree images.
\cite[p. 538]{hdrbook}

\paragraph*{}
All of these projections are static, showing the entirety of the 360\degree image at once. It is also possible to view 360\degree images interactively. In this case the field of view tends to be limited, so only a certain part of the image needs to be projected: the part of the image the viewer is ``facing'' virtually. Once the viewing direction has been determined, the projection can be calculated such that the center of the image has minimal distortion. Theoretically, any of the above projections could be used for this. 

\begin{figure}
\centering
    \hfill
    \begin{subfigure}[t]{0.5\textwidth}            
            \centering
            \includegraphics[width=0.7\textwidth, keepaspectratio]{02/mapping_cube_photo.jpg}
            \caption{Cube map example}\label{fig:cubemap-intro}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.7\textwidth, keepaspectratio]{02/mapping_cube.jpg}
            \caption{Cube map distortion visualization}\label{fig:cubemap-distortion}
  
    \end{subfigure}
    \hfill
    \par\bigskip % maximise vertical space here instead

    \hfill
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth, keepaspectratio]{02/mapping_sphere_photo.jpg}
            \caption{Mirrored sphere mapping example}
            \label{fig:sphere-intro}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth, keepaspectratio]{02/mapping_sphere.jpg}
            \caption{Mirrored sphere distortion visualization}
    \end{subfigure}
    \hfill
    \par\bigskip % maximise vertical space here instead

    \hfill
    \begin{subfigure}[t]{0.5\textwidth}            
            \centering
            \includegraphics[width=0.5\textwidth, keepaspectratio]{02/mapping_angular_photo.jpg}
            \caption{Angular mapping example}
            \label{fig:angular-intro}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth, keepaspectratio]{02/mapping_angular.jpg}
            \caption{Angular mapping distortion visualization}
    \end{subfigure}
    \hfill
    \par\bigskip % maximise vertical space here instead

    \hfill
    \begin{subfigure}[t]{0.5\textwidth} 
            \centering
            \includegraphics[width=0.9\textwidth, keepaspectratio]{02/mapping_latlong_photo.jpg}
            \caption{Equirectangular (latlong) mapping example}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.9\textwidth, keepaspectratio]{02/mapping_latlong.jpg}
            \caption{Equirectangular distortion visualization}\label{fig:latlong-intro}
    \end{subfigure}
    \hfill
    \caption{Common mappings for 360\degree images}\label{fig:common_mappings}
  \end{figure}
  
\subsection{Optical Flow} \label{subsec:optical_flow}
Optical flow describes the displacement of specific points between two images. It is generally used on consecutive frames of video sequences, for example for semantic segmentation, structure-from-motion, data compression or other applications where information about movement between images is required. To illustrate, Figure~\ref{fig:of_example_bike} shows two consecutive frames of a video sequence. On a high level, an optical flow algorithm should recognize that the pixels representing the bicyclist are moving towards the bottom left of the image, and the pixels representing the background are moving to the right (because the camera is panning slightly to the left).

There are two types of optical flow: Sparse optical flow and dense optical flow. Sparse optical flow algorithms calculate the motion of several select points that can be either chosen manually, or by some kind of automatic selection (e.g. based on features). This type of optical flow can be used to track only specific objects in a scene (e.g. the direction and relative velocity of a certain car in traffic).

Dense optical flow algorithms compute the motion of \emph{each pixel} between two images, instead of single points. This can be used for more general object tracking (e.g. direction and relative velocity of complete surroundings in traffic), to estimate 3D geometry (in structure-from-motion algorithms), or to identify static sections of the image for video compression \cite{of-survey}. Dense optical flow can also be used for image synthesis, such as in Richardt et al's Megastereo \cite{megastereo} described in Section~\ref{subsec:megastereo}, which is also the basis of the flow-based interpolation presented in this thesis.

There are a number of optical flow algorithms, ranging from methods using parametrization, or regularization \cite{of-survey}, to methods relying on Deep Learning \cite{of-deep}. Although these algorithms differ greatly in approach, they have in common the type of result, which is a vector field. For dense optical flow, this vector field contains a vector for each pixel, describing the displacement of this pixel between the input images. Sparse optical flow only contains a vector for each pre-chosen point, not for every pixel.

Figure~\ref{fig:of_vis} shows two different visualizations of the vector field calculated by the dense optical flow algorithm by Farneb\"ack \cite{farneback} between the frames in Figure~\ref{fig:of_example_bike}. Figure~\ref{fig:of_vis}a is a color-based visualization: the hue encodes the vector direction, the saturation encodes the vector length for each pixel. Using this visualization, it is possible to roughly distinguish two separately moving areas of the image, which could be used for semantic segmentation. Figure~\ref{fig:of_vis}b shows the pixel displacements with vectors: the origin of the vector is shown by a point and the direction and length of the vector are represented by a line.

These visualizations can help in understanding if and how well an optical flow algorithm is working. Although there are a large number of different algorithms, most of them still struggle with common issues such as occlusions, too-large displacements and intensity changes \cite{of-survey}. Occlusions are problematic, since the displacements between two images may reveal or cover image areas that, as a result, have no correspondence in the previous image. This problem is exacerbated when displacements are very large (e.g. due to fast-moving objects). Large displacements are also problematic by and of themselves, as most algorithms are not designed to handle them. How these limitations affect the use of optical flow for image synthesis will be explored in Chapters~\ref{chap:implementation} and \ref{chap:evaluation}.

\begin{figure}
\centering
    \begin{subfigure}[t]{0.5\textwidth}            
            \centering
            \includegraphics[width=0.9\textwidth, keepaspectratio]{02/of_example1.jpg}
            \caption{Frame 1}
    \end{subfigure}%
     %add desired spacing between images, e. g. ~, \quad, \qquad etc.
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.9\textwidth, keepaspectratio]{02/of_example2.jpg}
            \caption{Frame 2}
    \end{subfigure}
    \caption[Optical flow example]{Example frames that optical flow is calculated on}\label{fig:of_example_bike}

    \quad
    \begin{subfigure}[t]{0.5\textwidth}            
            \centering
            \includegraphics[width=0.9\textwidth, keepaspectratio]{02/of_vis1.jpg}
            \caption{Visualization of optical flow with color: the hue encodes the vector direction, the saturation encodes the vector length for each pixel.}
    \end{subfigure}%
     %add desired spacing between images, e. g. ~, \quad, \qquad etc.
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[t]{0.5\textwidth}
            \centering
            \includegraphics[width=0.9\textwidth, keepaspectratio]{02/of_vis2.jpg}
            \caption{Visualization of optical flow with vectors: the origin of the vector is shown by a small point (only a limited number of vectors are shown)}
    \end{subfigure}
    \caption[Optical flow visualizations]{Optical flow visualizations}\label{fig:of_vis}
\end{figure}

\section{Related Work}
Image-based Rendering (IBR) and viewpoint interpolation\footnotemark\ started gaining interest with the advent of virtual walkthroughs, for example for Apple's QuickTime\textsuperscript{\textregistered} VR, in order to save render time by generating views from images instead of using complex 3D scenes including textures, lights and complex geometric models \cite{quicktime}.
Chen and Zhang, in their survey on image-based rendering \cite{survey2004}, use the terms \emph{source description} and \emph{appearance description} to compare these basic rendering techniques: ``Traditional'' rendering techniques use \emph{source description}, i.e. the scene is described by the objects within it, their positions and properties. On the other hand, IBR techniques try to achieve the same goal through \emph{appearance description}. Vision itself has little to do with 3D geometry; it is the processing of a dense set of light rays by the brain which are ``captured'' by the eye. This process can also be performed by a capture device like a camera. So, instead of trying to describe the scene through the objects it contains, IBR rendering techniques try to model the \emph{light rays} that reach the viewer. 

\footnotetext{There seems to be no explicit difference between ``image-based rendering'', ``viewpoint interpolation'' and ``viewpoint synthesis'' in literature, so they will be used interchangeably in this thesis, although ``interpolation'' is favored for more simple blending techniques, and ``synthesis'' is used to describe more complex algorithms.}

While the differentiation between source description and appearance description is helpful in understanding the basic differences between image-based and traditional rendering, many image-based rendering methods also utilize some form of source description, predominantly in the form of 3D geometry. In different survey on image-based rendering techniques, Kang and Shum \cite{survey2000} classify IBR rendering techniques on a continuum, which ranges from techniques using no geometry, to techniques using implicit geometry (i.e. feature correspondences), to techniques using explicit geometry (see Figure~\ref{fig:survey_categorization}).

\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{02/shum_kang_survey.png}
    \caption[Categorization of IBR techniques from \cite{survey2000}]{Categorization of IBR techniques with representative members, taken from Kang and Shums ``A Review of Image-based Rendering Techniques''\cite{survey2000}}
		\label{fig:survey_categorization}
\end{figure}

The algorithm presented in this thesis is a combination of two different approaches: the first step uses no geometry whatsoever, and the second step uses feature correspondences to correct some problems that arise in the first step. This differentiation guides the related work presented here: The first section presents synthesis approaches using no geometry, and the second section presents approaches using implicit geometry or feature correspondences.
%Approaches using 360\degree images are preferred, but since these are less common than those using planar images, 
%Most research in the area of image synthesis is focused on planar images, but some 360\degree approaches can also be found.

%A theoretical model for \emph{appearance description} was developed by Adelson et al.\ \cite{Adelson91}: the \emph{plenoptic function} (Equation~\ref{eq:plenoptic}). The plenoptic function is a 7D function that describes the observable light at every point in space $V_x$, $V_y$, $V_z$, from every direction $\theta$, $\varphi$, at every wavelength $\lambda$, at every possible point in time $t$.

%\begin{equation}
%  \label{eq:plenoptic}
%  P = P(\theta, \varphi, \lambda, t, V_x, V_y, V_z)
%\end{equation}

%In practice, it is unfeasible, if not impossible, to cover all dimensions of this function, as this would require a capture device at every location, at every point in time, capturing light rays coming from every direction. However, by making assumptions, IBR techniques try to reconstruct simplified versions of the plenoptic function. Depending on which assumptions are made and how the surroundings are sampled, varying requirements are met and different results are achieved.

%Many of these approaches however, do use some geometry information in order to recalculate image points. The scene geometry is hereby either meticulously recorded at the time of the image capture, as with Kanade et al's 3D Dome \cite{geometry97}, or inferred from the image data alone, 

%The approach presented in this thesis is \emph{pixel-based}, i.e. using no geometry information, so the following section will first outline research aiming to synthesize viewpoints with the use of little to no geometry. Since the majority of pixel-based rendering research uses planar images, the second part will focus on research using 360\degree images, including approaches using geometry.

%\begin{itemize}
%  \item type of warping: uv mapping, warping based on triangulation and homography
%  \item image representation: cube, planar, sphere
%  \item constraints? color / angle / \ldots
%  \item geometry? sparse, dense, none
%  \item correspondences needed? sparse, dense, none
%  \item type of input: planar, 360, 180 pano
%  \item type of output: planar, 360, 180 pano
%\end{itemize}

\subsection{Image Synthesis with no Geometry}
\ldots light field rendering \cite{lightfield}

\begin{itemize}
  \item other approaches, such as light field approaches or neural network approaches
\end{itemize}

\subsubsection{``A Simple Method for Light Field Resampling'' \cite{simple_poster}}
Kawai \cite{simple_poster} approaches the problem of synthesizing new images with two degrees of freedom without using 3D geometry. Their basic setup is to capture four 360\degree images at each corner of a rectangular area and use resampling to synthesize a new image anywhere within this area.

The resampling is done by inserting a virtual sphere centered at the synthesized viewpoint representing a projection screen on which to project rays from the captured viewpoints. The locations of the captured viewpoints are known, so the outbound rays of these viewpoints can be calculated by using the image-to-world coordinate conversion from the equirectangular representation. The intersections of these captured rays with the virtual sphere are calculated and the corresponding pixel values used. The projection screen where no rays have intersected are approximated by repeating the reprojection with different resolutions.

In cases where several rays share an intersection, they use a rating based on the inner product of the ray direction and the viewing direction and use the ray with the smallest score. As an alternative, they suggest prioritizing one specific captured viewpoint over the others to completely avoid ghosting artefacts.

\subsubsection{``On the Use of Ray-tracing for Viewpoint Interpolation in Panoramic Imagery'' \cite{raytracing}}
%correspondences needed: none, geometry: none/dense, constraints: color-constraint, image rep: cube, type of warping: pixel blending
Shi et al.\ \cite{raytracing} examine how ray tracing can be used to calculate arbitrary new viewpoints based on knowledge of relative positions between the viewpoints which are stored as cube maps. For every pixel in the target image, a ray is cast into the scene. In order to find the correct value of that point, they use a color consistency constraint, which determines whether the pixel values of the reference images are similar. The assumption is that if the colors differ, the rays must not be intersecting the same point.

In order to calculate an intersection with the scene, they propose two different methods: A brute-force depth search using no scene geometry which searches along all of the captured rays until the pixel values are similar enough to fulfill their color constraint requirements, or a guided depth search using sparse 3D reconstruction.

To evaluate their method, they use a set of captured input images with a maximum distance of one meter, from which they remove one to use as ground truth. They evaluate the algorithm by describing the artefacts in the results and compare the brute-force with the guided depth search.

\subsubsection{``Unconstrained Segue Navigation for an Immersive Virtual Reality Experience'' \cite{segue}}
Herath et al.\ \cite{segue} propose a system that enables casual users to capture their surroundings with a smartphone in a grid and then navigate that environment with two degrees of freedom. In order to interpolate between two captured 360\degree images, they differentiate between faces that are parallel to the axis of movement and faces that are perpendicular to the axis of movement. For faces that are parallel, they stitch the faces of two adjacent viewpoints together and interpolate by using a sliding window. For faces that are perpendicular, they calculate a homography between the faces of two adjacent viewpoints and morph the image accordingly. To interpolate any image within a rectangular area bounded by four captured viewpoints, they recursively interpolate intermediary viewpoints until they reach the desired position.

Although they do not mention the dimensions of possible scenes, it must be assumed that the scenes are very large and objects very distant from the camera, since calculating a homography would not work given large perspective shifts.

%capture of scene in a grid
%1DoF interpolation:
%faces that are perpendicular \ar zoom (calculate homography between four corners and warp)
%faces that are parallel \ar stitch face from image 1 and two and slide window across depending on the distance travelled
%2DoF interpolation:
%extend for arbitrary points by using intermediate images
%not mentioned, but perspective shifts will not work at all \ar presumably only outdoor scenes with a large distance to the camera

\subsection{Image Synthesis with Implicit Geometry}
Leveraging image correspondences for synthesis has been a popular method almost since the beginning of viewpoint synthesis. Chen and Williams \cite{apple} were one of the first to use ``the morphing method'' that simultaneously blends the shape and texture of two images using image correspondences. A comparable method, based on optical flow, is used by Richardt et al.\ \cite{megastereo} for planar images.

Adaping planar algorithms (e.g. optical flow, structure-from-motion) for 360\degree images is a common challenge in 360\degree image synthesis. Kolhatkar et al.\ \cite{360flowblending} and Huang et al.\ \cite{6dof} solve this problem by extending the faces of the cube maps to account for pixels moving across borders. \cite{360flowblending} then use optical flow for interpolation between two images, whereas \cite{6dof} estimates the scene geometry with an SfM algorithm to extend monoscopic 360\degree videos to stereo. Zhao et al.\ \cite{cube2video} propose a method for adapting sparse correspondence matching for the spherical domain, circumventing the need to use an extended cube map.

Morphing the images to create a new viewpoint can be done with pixel-based blending \cite{megastereo}, \cite{360flowblending} or by triangulating the image and calculating homographies between the triangles \cite{6dof}, \cite{cube2video}.

%, and \cite{360flowblending} for 360\degree images, whose approaches are very similar to the flow-based blending step presented in this thesis.

%- triangulation based on correspondences + homographical morphing

The flow-based blending approach in this thesis builds on the approaches of \cite{megastereo} and \cite{360flowblending}, which are presented in more detail in the following sections.

\subsubsection{``Real-Time Virtual Viewpoint Generation on the GPU for Scene Navigation'' \cite{360flowblending}}
Kolhatkar and Lagani\`ere \cite{360flowblending} propose a method for smoothly interpolating between pairs of 360\degree images. Their approach leverages the optical flow calculated between 
Their approach is similar to the approach in Megastereo, where optical flow between the images is used to incrementally morph the two images. Since they use 360\degree images, they extend the cube map representation to account for points moving across edges, which is the method that is used in this thesis, as well. To reduce artefacts in the obtained optical flow, they perform a matching and smoothing step. They then implement their algorithm on the GPU, which allows them to interpolate between images in real-time. As to the maximum distances that are possible for interpolation, no definitive values are given, rather, ``reasonable'' distances are quoted.


\subsubsection{``Megastereo: Constructing High-Resolution Stereo Panoramas'' \cite{megastereo} \label{subsec:megastereo}}
%correspondences needed: dense, geometry: none, constraints: angle?, image rep: planar, type of warping: none, because planar
Richardt et al.\ \cite{megastereo} present an approach to combine planar images captured casually on a radius to create a panoramic image that is viewable in stereo in high resolution. In place of scene geometry, they use a cylindrical imaging surface that is concentric to the capture center. For each eye at a given viewing orientation, they project a ray into the scene (Figure~\ref{fig:megastereo}a) and calculate the deviation angle $\alpha$ between the desired ray and the nearest captured rays (Figure~\ref{fig:megastereo}b). 

\begin{figure}[]
\centering
\includegraphics[width=1\textwidth, keepaspectratio]{02/megastereo-figure.png}
\caption[Flow-based blending in Megastereo \cite{megastereo}]{(a) Illustration of rays required for creating a stereoscopic panorama and (b) deviation angles $\alpha$. (c) Duplication and truncation artefacts caused by the aliasing. (d) Flow-based upsampling to synthesize required rays. \emph{Adapted from \cite{megastereo}}}
\label{fig:megastereo}
\end{figure}

Linearly blending the rays of the two closest captures would lead to artefacts due to the difference between the real geometry and the cylindrical surface (Figure~\ref{fig:megastereo}c), and using a nearest-neighbor technique results in discontinuities. Instead, they propose a ``flow-based blending'' technique: For each ray of the final image that is not a captured ray (deviation angle 0), a new ray is synthesized using optical flow. The vertical image strip captured by the synthesized ray is interpolated by taking the two closest viewpoints $I_K$ and $I_L$ and interpolating $\widetilde{I_M}$ (Figure~\ref{fig:megastereo}d) using the optical flow vectors $F_{k\rightarrow l}$ and $F_{l\rightarrow k}$.  The corresponding strip is then taken from this new viewpoint which contains the matching ray.

%use a combination of image stitching and their own optical flow-based blending algorithm in order to account for . The goal is to synthesize stereoscopic viewpoints (an image for the left and right eye, each) from the set of captured monoscopic images. For the image strips for which a matching ray was not captured, they introduce a blending algorithm based on optical flow.

%After transforming the images so that they all have scene-independent orientation and minimal distortion, strips of the captured images are extracted for each ``eye'' based on the best matching image ray (Figure~\ref{fig:megastereo-figure}a and b). In cases where there is no perfect ray correspondence, the ray with the smallest deviation angle can be chosen, however, this can lead to artefacts as illustrated in Figure~\ref{fig:megastereo-figure}c. In order to mitigate these artefacts, Richardt et al.\ introduce a blending algorithm based on optical flow: 

%For Megastereo, there is no need to blend the complete images, instead, they restrict their calculations to the strips/pixels they need. For simplicity's sake, the process is described for a full image:
The interpolated image $\widetilde{I_M}$ at point $\eta$ between the images $I_K$ and $I_L$ is calculated by shifting $I_K$ by $\eta \cdot F_{k\rightarrow l}$ and by shifting $I_L$ by $(1 - \eta) \cdot F_{l\rightarrow k}$. The two shifted images are then blended linearly, using $\eta$ as the weight. Instead of calculating the entire image for each ray, only the necessary image areas are extracted and the interpolation is calculated pixel-wise.

According to the authors, their approach ``can handle angular resolutions from 1\degree to 4\degree, but even 8\degree produces agreeable results''. Although the approach is designed for planar images, the idea of leveraging deviation angles for  flow-blending approach is used 

%\subsubsection{6-DOF VR Videos with a Single 360-Camera}
%%correspondences needed: none, geometry: dense, constraints: none, image rep: cube, type of warping: triangulation
%Huang et al.\ \cite{6dof} propose a method to expand regular 360\degree video data into stereoscopic video data viewable with six degrees of freedom. Their approach is based on reconstructing the 3D scene and the camera path.
%
%This reconstruction is achieved by adapting well-established structure-from-motion (SfM) algorithms for 360\degree data by the same method as \cite{360flowblending}, by extending the cube map.
%Since SfM algorithms are designed for planar images with a limited field of view (FoV), the six separate faces of the cube map representation are used as input for the SfM algorithm. SfM algorithms work by tracking points across a series of images (e.g. frames), so for cube mappings, potential points moving across seams need to be accounted for. This is done by extending the field of view of each face so that regions around seams are represented on both sides of the seam. After the points are correctly tracked, the extended cube map is reduced back to its original format.

%The sparse scene geometry is inferred from the SfM data and then refined to a dense reconstruction using depth maps and triangulation. Finally, new viewpoints are synthesized by warping the current frame by using triangulation and warping the triangles (see Figure~\ref{fig:6dof}).
%Then, the sparse scene geometry and camera path are calculated by incrementation and interpolation. The sparse geometry is then refined to a dense reconstruction of the scene using depth maps and Delaunay Triangulation. Finally, new viewpoints are synthesized by warping the current frame. Their warping algorithm projects the dense 3D points to a unit sphere control mesh with icosahedral tesselation\footnote{Icosahedral tesselation of a sphere subdivides the surface of a sphere into triangles. Other terms for this type of sphere are Icosphere or Geodesic Polyhedron} for both the captured viewpoint and the desired viewpoint. Then the vertex motion between each pair of triangles is calculated and the image warped according to these motion vectors (see Figure~\ref{6dof-figure}). 

%\begin{figure}[]
%\centering
%\includegraphics[width=0.6\textwidth, keepaspectratio]{02/6dof-figure.jpg}
%\caption[Image warping from Huang et al. \cite{6dof}]{The warping field is computed by finding the movement between the projections of each 3D scene point in the reference and the synthesized frame. Then the warping field is used to map each pixel of the synthesized image to its corresponding pixel in the reference to look up its color information. \emph{Adapted from \cite{6dof}}}
%\label{fig:6dof}
%\end{figure}


